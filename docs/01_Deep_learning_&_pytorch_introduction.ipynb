{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YugyeongJo/SeSAC_DL_Study/blob/main/docs/01_Deep_learning_%26_pytorch_introduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pytorch 설치 및 확인\n"
      ],
      "metadata": {
        "id": "7R1ybKJ7JUK7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYC7h8bMATYj",
        "outputId": "cfe65db5-28d4-48ba-957b-12dcaa5a52ee"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.__version__)\n",
        "print(torch.cuda.is_available())"
      ],
      "metadata": {
        "id": "v_C7Xg6KJjm7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf6b7f56-dc52-456a-c980-c2efb9881b96"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.4.1+cu121\n",
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pytorch Tensor\n",
        "\n",
        "pytorch의 가장 근본이 되는 Tensor들에 대해서 배워보겠습니다.\n",
        "\n",
        "### Tensor 만드는 법\n",
        "\n",
        "\n",
        "torch.tensor(data): data는 튜플, 리스트, numpy 배열 등등임.\n",
        "\n",
        "주요 속성들\n",
        "- dtype: 데이터 타입\n",
        "- device: gpu에 있는지, cpu에 있는지\n",
        "- requires_grad: 이게 True면 미분값을 계산함. 아니면 하지 않음.\n"
      ],
      "metadata": {
        "id": "WAIaC3rWAp3O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 0-D Tensor\n",
        "scalar = torch.tensor(5.0)\n",
        "print(scalar)  # tensor(5.)\n",
        "\n",
        "vector = torch.tensor([1.0, 2.0, 3.0])\n",
        "print(vector)  # tensor([1., 2., 3.])\n",
        "\n",
        "matrix = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
        "print(matrix)\n",
        "# tensor([[1., 2.],\n",
        "#         [3., 4.]])\n",
        "\n",
        "tensor_3d = torch.tensor([[[1.0], [2.0]], [[3.0], [4.0]]])\n",
        "print(tensor_3d)\n",
        "# tensor([[[1.],\n",
        "#          [2.]],\n",
        "#\n",
        "#         [[3.],\n",
        "#          [4.]]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnOILb3NGJrx",
        "outputId": "4ac47af6-e8bd-4bfb-d4f6-855019910a88"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(5.)\n",
            "tensor([1., 2., 3.])\n",
            "tensor([[1., 2.],\n",
            "        [3., 4.]])\n",
            "tensor([[[1.],\n",
            "         [2.]],\n",
            "\n",
            "        [[3.],\n",
            "         [4.]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## My Code\n",
        "\n",
        "# 0차원 = scalar\n",
        "number = torch.tensor(1.0)\n",
        "print(\"number : \", number)\n",
        "\n",
        "# 1차원 = vector\n",
        "lst_vector = torch.tensor([1, 2, 3])\n",
        "tuple_vector = torch.tensor((1, 2, 3))\n",
        "print(\"lst_vector : \", lst_vector)\n",
        "print(\"tuple_vector : \", tuple_vector)\n",
        "\n",
        "# 2차원 = matrix\n",
        "matrix_2d = torch.tensor([[1, 2, 3], [3, 4, 5]])\n",
        "print(\"matrix_2d size : \", matrix_2d.shape)\n",
        "\n",
        "# 3차원 = matrix\n",
        "matrix_2 = [[1, 2, 3], [3, 4, 5]]\n",
        "lst = [matrix_2, matrix_2, matrix_2]\n",
        "matrix_2_3d = torch.tensor(lst)\n",
        "print(\"matrix_2_3d size : \", matrix_2_3d.shape)\n",
        "\n",
        "### 차원 = list of list of list ...의 list 갯수\n",
        "### shape = 원소의 갯수"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qWsUbYnoShP",
        "outputId": "c8aaf8cb-dc69-4eb8-bd7b-b02067a211c1"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number :  tensor(1.)\n",
            "lst_vector :  tensor([1, 2, 3])\n",
            "tuple_vector :  tensor([1, 2, 3])\n",
            "matrix_2d size :  torch.Size([2, 3])\n",
            "matrix_2_3d size :  torch.Size([3, 2, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### torch.tensor의 주요 속성들\n",
        "\n",
        "- tensor.shape\n",
        "- tensor.size()\n",
        "- tensor.dtype"
      ],
      "metadata": {
        "id": "3cMM5BWTmTSF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(vector.shape)    # torch.Size([3])\n",
        "print(matrix.size())   # torch.Size([2, 2])\n",
        "print(tensor_3d.shape) # torch.Size([2, 2, 1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJg4nrPAMt7t",
        "outputId": "2568db8d-4766-4964-e834-5b738053e0ef"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3])\n",
            "torch.Size([2, 2])\n",
            "torch.Size([2, 2, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(vector.dtype)    # torch.float32\n",
        "int_tensor = torch.tensor([1, 2, 3], dtype=torch.int32)\n",
        "print(int_tensor.dtype)  # torch.int32\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "tensor_gpu = torch.tensor([1.0, 2.0, 3.0]).to(device)\n",
        "print(tensor_gpu.device)  # cuda:0 or cpu\n",
        "\n",
        "## GPU간의 옮기고, CPU간의 옮기는 거 시간 오래 걸림\n",
        "## 연산은 같은 곳에 있어야 가능"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrqNxnpxMwE3",
        "outputId": "57127f17-6e68-478f-c79c-ff026f9ddbdb"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.float32\n",
            "torch.int32\n",
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### torch.tensor 만드는 방법\n",
        "\n",
        "- torch.tensor(data)\n",
        "- 자주 쓰는 텐서들은 만드는 함수가 있음.\n",
        "  * torch.zeros(size): size 형태로 된, 0으로 된 텐서를 만듬.\n",
        "  * torch.ones(size): size 형태로 된, 1로 된 텐서를 만듬.\n",
        "  * torch.rand(size) / torch.randn(size) : 랜덤한 숫자로 된 텐서를 만듬. rand는 0과 1 사이에서 랜덤하게, randn은 표준정규분포(평균 0, 표준편차 1)에서 뽑아옴.\n",
        "  * torch.eye(n): 대각선만 1이고 이외에는 0인 2D 텐서(행렬)을 만듬.\n",
        "- 이외에도 많이 쓰이는 함수들\n",
        "  * torch.arange: range() 함수와 매우 비슷하다.\n",
        "  * torch.linspace(start, end, steps): start부터, end까지, steps개의 숫자를 가지는 텐서를 만듬. 이 때, 숫자들은 등간격으로 만들어짐.\n"
      ],
      "metadata": {
        "id": "_GXHR4BemnJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_tensor = torch.tensor([1, 2, 3])\n",
        "tuple_tensor = torch.tensor((4, 5, 6))\n",
        "\n",
        "zeros = torch.zeros((2, 3))\n",
        "ones = torch.ones((2, 3))\n",
        "rand = torch.rand((2, 3))\n",
        "eye = torch.eye(3)  # 3x3 Identity matrix\n",
        "\n",
        "normal = torch.randn((2, 3))  # Normal distribution\n",
        "\n",
        "arange_tensor = torch.arange(start=0, end=10, step=2)\n",
        "linspace_tensor = torch.linspace(start=0, end=1, steps=5)\n",
        "\n",
        "float_tensor = torch.tensor([1.0, 2.0, 3.0], dtype=torch.float64)\n",
        "int_tensor = torch.tensor([1, 2, 3], dtype=torch.int32)\n"
      ],
      "metadata": {
        "id": "HuUTrq-ekUmh"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## My Code\n",
        "\n",
        "zeros = torch.zeros((2, 3))\n",
        "ones = torch.ones((2, 3))\n",
        "rand = torch.rand((2, 3))\n",
        "eye = torch.eye(3)\n",
        "print(zeros)\n",
        "print(ones)\n",
        "print(rand)\n",
        "print(eye)\n",
        "\n",
        "arange_tensor = torch.arange(start=0, end=10, step=2)\n",
        "linspace_tensor = torch.linspace(start=0, end=1, steps=5)\n",
        "print(arange_tensor)\n",
        "print(linspace_tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqo0bxoCwuN7",
        "outputId": "b2c0d592-be3c-4e56-ce87-3b69139316e1"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n",
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.]])\n",
            "tensor([[0.5341, 0.8204, 0.9470],\n",
            "        [0.0773, 0.5602, 0.9947]])\n",
            "tensor([[1., 0., 0.],\n",
            "        [0., 1., 0.],\n",
            "        [0., 0., 1.]])\n",
            "tensor([0, 2, 4, 6, 8])\n",
            "tensor([0.0000, 0.2500, 0.5000, 0.7500, 1.0000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO: 위 각 함수들을 torch.tensor와 파이썬 리스트 operation들을 이용하여 재구현해 보세요."
      ],
      "metadata": {
        "id": "k6Pf1-B3vOVS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write your code here\n",
        "\n",
        "def torch_zeros(*size):\n",
        "  if isinstance(size[0], (tuple, list)):\n",
        "    size = size[0]\n",
        "\n",
        "  elif isinstance(size[0], int):\n",
        "    for x in size:\n",
        "      if not isinstance(x, int):\n",
        "        raise ValueError(\"only input int type\")\n",
        "    size = tuple(size)\n",
        "\n",
        "  def create_zeros(dimensions):\n",
        "    if len(dimensions) == 1:\n",
        "      return [0 for _ in range(dimensions[0])]\n",
        "    else:\n",
        "      return [create_zeros(dimensions[1:]) for _ in range(dimensions[0])]\n",
        "\n",
        "  res = create_zeros(size)\n",
        "\n",
        "  return torch.tensor(res)\n",
        "\n",
        "# answer = torch_zeros(2, 5, 3)\n",
        "# print(answer)\n",
        "\n",
        "def torch_ones(*size):\n",
        "  if isinstance(size[0], (tuple, list)):\n",
        "    size = size[0]\n",
        "\n",
        "  elif isinstance(size[0], int):\n",
        "    for x in size:\n",
        "      if not isinstance(x, int):\n",
        "        raise ValueError(\"only input int type\")\n",
        "    size = tuple(size)\n",
        "\n",
        "  def create_zeros(dimensions):\n",
        "    if len(dimensions) == 1:\n",
        "      return [1 for _ in range(dimensions[0])]\n",
        "    else:\n",
        "      return [create_zeros(dimensions[1:]) for _ in range(dimensions[0])]\n",
        "\n",
        "  res = create_zeros(size)\n",
        "\n",
        "  return torch.tensor(res)\n",
        "\n",
        "answer = torch_ones(2, 5, 3)\n",
        "print(answer)\n",
        "\n",
        "def torch_rand():\n",
        "  return ''\n",
        "\n",
        "def torch_eye():\n",
        "  return ''\n",
        "\n",
        "def torch_arange():\n",
        "  return ''\n",
        "\n",
        "def torch_linspace():\n",
        "  return ''"
      ],
      "metadata": {
        "id": "mUiK8UuXpHfE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3e985d5-2a58-4001-e2dc-b4c42fe3534a"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[1, 1, 1],\n",
            "         [1, 1, 1],\n",
            "         [1, 1, 1],\n",
            "         [1, 1, 1],\n",
            "         [1, 1, 1]],\n",
            "\n",
            "        [[1, 1, 1],\n",
            "         [1, 1, 1],\n",
            "         [1, 1, 1],\n",
            "         [1, 1, 1],\n",
            "         [1, 1, 1]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Solution\n",
        "import torch\n",
        "import random\n",
        "\n",
        "def nested_list(shape, value = 0):\n",
        "  \"\"\"Accepts tuple/list of int, denoting shape of the nested list.\n",
        "  \"\"\"\n",
        "  if len(shape) == 1:\n",
        "    I = shape[0]\n",
        "    return [value for _ in range(I)]\n",
        "  else:\n",
        "    I = shape[0]\n",
        "    return [nested_list(shape[1:], value = value) for _ in range(I)]\n",
        "\n",
        "def random_nested_list(shape):\n",
        "  \"\"\"Accepts tuple/list of int, denoting shape of the nested list.\n",
        "  \"\"\"\n",
        "  if len(shape) == 1:\n",
        "    I = shape[0]\n",
        "    return [random.random() for _ in range(I)]\n",
        "  else:\n",
        "    I = shape[0]\n",
        "    return [random_nested_list(shape[1:]) for _ in range(I)]\n",
        "\n",
        "def randomn_nested_list(shape):\n",
        "  \"\"\"Accepts tuple/list of int, denoting shape of the nested list.\n",
        "  \"\"\"\n",
        "  if len(shape) == 1:\n",
        "    I = shape[0]\n",
        "    return [random.gauss(0, 1) for _ in range(I)]\n",
        "  else:\n",
        "    I = shape[0]\n",
        "    return [randomn_nested_list(shape[1:]) for _ in range(I)]\n",
        "\n",
        "def zeros(shape):\n",
        "  return torch.tensor(nested_list(shape, value = 0))\n",
        "\n",
        "def ones(shape):\n",
        "  return torch.tensor(nested_list(shape, value = 1))\n",
        "\n",
        "def rand(shape):\n",
        "  return torch.tensor(random_nested_list(shape))\n",
        "\n",
        "def randn(shape):\n",
        "  return torch.tensor(randomn_nested_list(shape))\n",
        "\n",
        "print(nested_list((2, 3, 4), value = 0))\n",
        "print(zeros((1, 2, 3)))\n",
        "print(ones((1, 2, 3)))\n",
        "print(rand((1, 2, 3)))\n",
        "print(randn((1, 2, 3)))\n",
        "\n",
        "def eyes(n):\n",
        "  lst = [[0 for i in range(n)] for j in range(n)]\n",
        "\n",
        "  for i in range(n):\n",
        "    lst[i][i] - 1\n",
        "\n",
        "  return torch.tensor(lst)\n",
        "\n",
        "print(eyes(4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6wk6p0f-pM-",
        "outputId": "a05a9d75-71aa-461c-ccab-fdfc4bad8bd2"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]], [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]]]\n",
            "tensor([[[0, 0, 0],\n",
            "         [0, 0, 0]]])\n",
            "tensor([[[1, 1, 1],\n",
            "         [1, 1, 1]]])\n",
            "tensor([[[0.0250, 0.6157, 0.3255],\n",
            "         [0.0221, 0.8931, 0.3542]]])\n",
            "tensor([[[-0.0020, -0.4861, -0.0054],\n",
            "         [-0.2160,  0.4968,  1.0854]]])\n",
            "tensor([[0, 0, 0, 0],\n",
            "        [0, 0, 0, 0],\n",
            "        [0, 0, 0, 0],\n",
            "        [0, 0, 0, 0]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### torch.tensor끼리의 연산\n",
        "\n",
        "일반적인 사칙연산, 행렬 곱(matmul), 원소간 곱 등등이 다 적용됨."
      ],
      "metadata": {
        "id": "PWpAlToLo7wq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([1.0, 2.0, 3.0])\n",
        "b = torch.tensor([4.0, 5.0, 6.0])\n",
        "\n",
        "add = a + b  # tensor([5., 7., 9.])\n",
        "sub = a - b  # tensor([-3., -3., -3.])\n",
        "\n",
        "mul = a * b  # tensor([ 4., 10., 18.])\n",
        "div = b / a  # tensor([4.0000, 2.5000, 2.0000])\n",
        "\n",
        "exp = a ** 2  # tensor([1., 4., 9.])"
      ],
      "metadata": {
        "id": "JsvDLKcKkd2e"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "matrix_a = torch.tensor([[1, 2], [3, 4]])\n",
        "matrix_b = torch.tensor([[5, 6], [7, 8]])\n",
        "\n",
        "matmul = torch.matmul(matrix_a, matrix_b)\n",
        "# tensor([[19, 22],\n",
        "#         [43, 50]])\n",
        "\n",
        "elem_mul = matrix_a * matrix_b\n",
        "# tensor([[ 5, 12],\n",
        "#         [21, 32]])\n",
        "\n",
        "transposed = torch.transpose(matrix_a, 0, 1)\n",
        "# tensor([[1, 3],\n",
        "#         [2, 4]])\n",
        "\n",
        "print(torch.matmul(torch.tensor([[1], [2], [3]]), torch.tensor([[4, 5, 6]])))\n"
      ],
      "metadata": {
        "id": "eAa8J66qkjhp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c43e33d9-8086-4876-a782-306680a8d979"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 4,  5,  6],\n",
            "        [ 8, 10, 12],\n",
            "        [12, 15, 18]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Broadcasting\n",
        "\n",
        "브로드캐스팅은 서로 다른 크기를 가진 텐서들 간에 연산을 수행할 때, 자동으로 크기를 맞춰주는 PyTorch(및 NumPy)의 기능입니다. 이 기능은 명시적으로 텐서의 크기를 변환하지 않아도, 작은 크기의 텐서를 큰 크기의 텐서와 함께 연산할 수 있도록 해줍니다. Pandas나 Numpy 등에서도 자주 활용되기 때문에 알아두면 좋습니다.\n",
        "\n",
        "브로드캐스팅 규칙:\n",
        "1. 차원의 맞추기: 두 텐서의 차원(Dimension) 수가 다를 때, 차원이 작은 텐서의 앞쪽에 1을 추가하여 차원을 맞춥니다.\n",
        "2. 크기 맞추기: 각 차원에서 크기가 1인 텐서는 해당 차원의 크기를 큰 텐서의 크기에 맞춰 늘릴 수 있습니다.\n",
        "3. 불가능한 경우: 두 텐서가 특정 차원에서 서로 다른 크기를 가지며, 그중 하나가 1이 아니면 브로드캐스팅이 불가능하고 오류가 발생합니다.\n",
        "\n",
        "예를 들어서,\n",
        "\n",
        "- (2,3) 크기의 텐서에 (3,) 크기의 텐서를 더하면, (2,3) 크기의 텐서가 됩니다. 이 때 (3,) 크기의 텐서들은 첫 번째 차원에 대해서 다 더해집니다.\n"
      ],
      "metadata": {
        "id": "Z5Mjhn93pEQ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([[1, 2, 3], [4, 5, 6]])  # Shape: (2, 3)\n",
        "b = torch.tensor([1, 2, 3])              # Shape: (3,)\n",
        "\n",
        "broadcast_add = a + b  # Shape: (2, 3)\n",
        "# tensor([[2, 4, 6],\n",
        "#         [5, 7, 9]])\n",
        "\n",
        "a = torch.tensor([[1], [2], [3]])  # Shape: (3, 1)\n",
        "b = torch.tensor([4, 5, 6])        # Shape: (3,)\n",
        "\n",
        "# To make shapes compatible:\n",
        "# a: (3, 1) -> (3, 3)\n",
        "# b: (3,)   -> (1, 3) -> (3, 3)\n",
        "\n",
        "broadcast_mul = a * b  # Shape: (3, 3)\n",
        "# tensor([[ 4,  5,  6],\n",
        "#         [ 8, 10, 12],\n",
        "#         [12, 15, 18]])"
      ],
      "metadata": {
        "id": "VolcxkK_krYh"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## My Code\n",
        "\n",
        "a = torch.tensor([[1], [2], [3]])\n",
        "b = torch.tensor([4, 5, 6])\n",
        "# a = torch.tensor([[1, 1, 1], [2, 2, 2], [3, 3, 3]])\n",
        "# b = torch.tensor([[4, 5, 6], [4, 5, 6], [4, 5, 6]])"
      ],
      "metadata": {
        "id": "bJEiDuK7PmtN"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 이 외 tensor operation들\n",
        "\n",
        "- Slicing / Indexing\n",
        "- Reshaping\n",
        "- Concatenation / Stacking"
      ],
      "metadata": {
        "id": "xN5jjYflr1oE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# slicing / indexing\n",
        "tensor = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
        "\n",
        "# Basic indexing\n",
        "element = tensor[1, 2]  # tensor(6)\n",
        "\n",
        "# Slicing\n",
        "sub_tensor = tensor[:, 1:]  # tensor([[2, 3],\n",
        "                            #         [5, 6],\n",
        "                            #         [8, 9]])\n",
        "\n",
        "# Advanced indexing with masks\n",
        "mask = tensor > 5\n",
        "filtered = tensor[mask]  # tensor([6, 7, 8, 9])"
      ],
      "metadata": {
        "id": "Jz0GLy02kt-Y"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reshaping\n",
        "\n",
        "tensor = torch.arange(0, 12)\n",
        "reshaped_view = tensor.view(3, 4)  # tensor([[ 0,  1,  2,  3],\n",
        "                                   #         [ 4,  5,  6,  7],\n",
        "                                   #         [ 8,  9, 10, 11]])\n",
        "\n",
        "reshaped_reshape = tensor.reshape(2, 6)  # tensor([[ 0,  1,  2,  3,  4,  5],\n",
        "                                         #         [ 6,  7,  8,  9, 10, 11]])\n",
        "\n",
        "# tensor.permute\n",
        "tensor = torch.randn(2, 3, 4)\n",
        "permuted = tensor.permute(2, 0, 1)  # Changes the order of dimensions\n",
        "print(permuted.shape)  # torch.Size([4, 2, 3])\n"
      ],
      "metadata": {
        "id": "UiCkGFvjsnBq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0301898-e034-4907-a7eb-46a20b034af2"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 2, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([1, 2, 3])\n",
        "b = torch.tensor([4, 5, 6])\n",
        "\n",
        "# Concatenate along existing dimension\n",
        "concat = torch.cat((a, b), dim=0)  # tensor([1, 2, 3, 4, 5, 6])\n",
        "\n",
        "# Stack along a new dimension\n",
        "stack = torch.stack((a, b), dim=0)\n",
        "# tensor([[1, 2, 3],\n",
        "#         [4, 5, 6]])\n"
      ],
      "metadata": {
        "id": "VIHxbxPTs4Yx"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 수학적 함수들\n",
        "\n",
        "- abs, sqrt, exp, log 등 unary 함수들 (텐서 하나만을 input으로 받음): torch.abs, torch.sqrt, torch.exp, torch.log\n",
        "- max, min 등 binary 함수들 (텐서 2개를 input으로 받음): torch.max, torch.min\n",
        "- 차원을 하나 혹은 여럿 낮추는 Reduction Operation들: torch.sum(tensor, dim = n)\n"
      ],
      "metadata": {
        "id": "YCXr7r3Kr__Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([-1.0, -2.0, 3.0])\n",
        "\n",
        "abs_a = torch.abs(a)          # tensor([1., 2., 3.])\n",
        "sqrt_a = torch.sqrt(torch.abs(a))  # tensor([1., 1.4142, 1.7321])\n",
        "exp_a = torch.exp(a)          # tensor([0.3679, 0.1353, 20.0855])\n",
        "# 자연상수\n",
        "log_a = torch.log(torch.abs(a))    # tensor([0.0000, 0.6931, 1.0986])\n"
      ],
      "metadata": {
        "id": "UsFdYgJws8kL"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([1.0, 2.0, 3.0])\n",
        "b = torch.tensor([4.0, 5.0, 6.0])\n",
        "\n",
        "max_ab = torch.max(a, b)  # tensor([4., 5., 6.])\n",
        "min_ab = torch.min(a, b)  # tensor([1., 2., 3.])\n"
      ],
      "metadata": {
        "id": "EYIgfoa7s-gX"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tensor = torch.tensor([[1, 2], [3, 4]])\n",
        "\n",
        "sum_all = torch.sum(tensor)          # tensor(10)\n",
        "sum_dim0 = torch.sum(tensor, dim=0)  # tensor([4, 6])\n",
        "sum_dim1 = torch.sum(tensor, dim=1)  # tensor([3, 7])\n",
        "\n",
        "mean_all = torch.mean(tensor.float())  # tensor(2.5000)\n"
      ],
      "metadata": {
        "id": "lmMzGqgOs_kk"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([1, 2, 3])\n",
        "b = torch.tensor([2, 2, 2])\n",
        "\n",
        "greater = a > b  # tensor([False, False, True])\n",
        "equal = a == b   # tensor([False, True, False])\n"
      ],
      "metadata": {
        "id": "dLsS1TKhtAn2"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## My Code\n",
        "\n",
        "tensor = torch.tensor([[1, 2, 3], [3, 4, 5]])\n",
        "\n",
        "sum_all = torch.sum(tensor)\n",
        "sum_dim0 = torch.sum(tensor, dim=0)\n",
        "sum_dim1 = torch.sum(tensor, dim=1)\n",
        "\n",
        "mean_all = torch.mean(tensor.float())\n",
        "\n",
        "# print(sum_all)\n",
        "# print(sum_dim0)\n",
        "# print(sum_dim1)\n",
        "# print(mean_all)\n",
        "\n",
        "a = torch.tensor([1.0, 2.0, 3.0])\n",
        "b = torch.tensor([4.0, 5.0, 6.0])\n",
        "max_a = torch.max(a)\n",
        "max_ab = torch.max(a, b)\n",
        "\n",
        "# print(max_a)\n",
        "# print(max_ab)"
      ],
      "metadata": {
        "id": "Vu_iVqJmU0kE"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pytorch로 다시 해 보는 선형회귀\n",
        "\n",
        "주어진 데이터 $(x_i, y_i)$ 에 대해서 $y=wx+b$에서, 가장 적절한 w와 b를 찾는 것이 선형회귀였음.\n",
        "\n",
        "y = wx + b 에서, w와 b는 parameter이고 x는 입력, y는 출력임.\n",
        "이 때 w랑 b를 구하기 위해서, 다음의 loss function을 최소화하는 방향으로 학습하고 싶다고 하자.\n",
        "\n",
        "$ MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $\n",
        "\n",
        "원래는 저 값을 그냥 바로 식으로 계산할 수 있었지만, 언제나 그렇지는 않기 때문에 (선형회귀 외의 다른 모델들에서) 수치적으로 계산해보자.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Wo5udXG_t6Jv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 임의로 데이터를 한번 만들어 보자.\n",
        "# True parameters\n",
        "true_w = 2.0\n",
        "true_b = 1.0\n",
        "\n",
        "# Generate data\n",
        "X = torch.randn(100, 1) * 10  # 100 samples, single feature\n",
        "y = true_w * X + true_b + torch.randn(100, 1) * 2  # Add noise\n",
        "\n",
        "# requires_grad = True로 해야 학습이 가능\n",
        "w = torch.randn(1, 1, requires_grad=True)\n",
        "b = torch.randn(1, requires_grad=True)\n",
        "\n",
        "learning_rate = 0.005\n",
        "epochs = 1000\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # Forward pass: compute predicted y\n",
        "    y_pred = X * w + b\n",
        "\n",
        "    # Compute and print loss\n",
        "    loss = torch.mean((y_pred - y) ** 2)\n",
        "\n",
        "    # Backward pass: compute gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # Update parameters using gradient descent\n",
        "    with torch.no_grad():\n",
        "        w -= learning_rate * w.grad\n",
        "        b -= learning_rate * b.grad\n",
        "\n",
        "    # Zero gradients after updating\n",
        "    w.grad.zero_()\n",
        "    b.grad.zero_()\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f'Epoch {epoch}: w = {w.item():.4f}, b = {b.item():.4f}, loss = {loss.item():.4f}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UeaB0-MIwM3h",
        "outputId": "4bd0df99-3581-4498-cd9a-05dd72d19a72"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: w = 2.7871, b = -1.4974, loss = 1657.1796\n",
            "Epoch 100: w = 1.9798, b = -0.1466, loss = 4.5952\n",
            "Epoch 200: w = 1.9812, b = 0.3475, loss = 4.0586\n",
            "Epoch 300: w = 1.9817, b = 0.5285, loss = 3.9866\n",
            "Epoch 400: w = 1.9819, b = 0.5949, loss = 3.9769\n",
            "Epoch 500: w = 1.9820, b = 0.6192, loss = 3.9756\n",
            "Epoch 600: w = 1.9820, b = 0.6281, loss = 3.9754\n",
            "Epoch 700: w = 1.9820, b = 0.6313, loss = 3.9754\n",
            "Epoch 800: w = 1.9820, b = 0.6325, loss = 3.9754\n",
            "Epoch 900: w = 1.9820, b = 0.6330, loss = 3.9754\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO: 위 선형회귀 부분을 함수로 만들고, 다양한 하이퍼파라미터 (여기서의 hyperparameter은 learning rate 뿐임)를 바꿔가며 최적의 모델을 찾아보세요.\n"
      ],
      "metadata": {
        "id": "Y2IbgzEsuXLv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write your code here\n",
        "import torch\n",
        "\n",
        "# 선형 회귀 학습 함수 (에포크별로 학습)\n",
        "def train_linear_regression(X, y, learning_rate, epochs=1000):\n",
        "    # 학습 가능한 파라미터 초기화\n",
        "    w = torch.randn(1, 1, requires_grad=True)\n",
        "    b = torch.randn(1, requires_grad=True)\n",
        "    best_loss = float('inf')\n",
        "    best_epoch = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # 예측값 계산\n",
        "        y_pred = X * w + b\n",
        "\n",
        "        # 손실 함수 계산 (평균 제곱 오차)\n",
        "        loss = torch.mean((y_pred - y) ** 2)\n",
        "\n",
        "        # 역전파로 그라디언트 계산\n",
        "        loss.backward()\n",
        "\n",
        "        # 그라디언트 하강법으로 파라미터 업데이트\n",
        "        with torch.no_grad():\n",
        "            w -= learning_rate * w.grad\n",
        "            b -= learning_rate * b.grad\n",
        "\n",
        "        # 업데이트 후 그라디언트 초기화\n",
        "        w.grad.zero_()\n",
        "        b.grad.zero_()\n",
        "\n",
        "        # 최적의 손실값과 에포크 저장\n",
        "        if loss.item() < best_loss:\n",
        "            best_loss = loss.item()\n",
        "            best_epoch = epoch\n",
        "\n",
        "    return w.item(), b.item(), best_loss, best_epoch\n",
        "\n",
        "# 최적의 학습률과 에포크 찾는 함수\n",
        "def find_best_learning_rate_and_epoch(X, y, learning_rates, epoch_range=1000):\n",
        "    best_lr = None\n",
        "    best_epoch = None\n",
        "    best_loss = float('inf')  # 무한대로 초기화\n",
        "    best_w, best_b = None, None\n",
        "\n",
        "    for lr in learning_rates:\n",
        "        print(f\"\\n학습률: {lr}로 학습 중\")\n",
        "        w, b, loss, epoch = train_linear_regression(X, y, learning_rate=lr, epochs=epoch_range)\n",
        "        print(f\"최종 파라미터: w = {w}, b = {b}, 최종 손실 = {loss:.4f}, 최적 에포크 = {epoch}\")\n",
        "\n",
        "        # 손실값이 가장 낮은 학습률 및 에포크 찾기\n",
        "        if loss < best_loss:\n",
        "            best_loss = loss\n",
        "            best_lr = lr\n",
        "            best_w, best_b = w, b\n",
        "            best_epoch = epoch\n",
        "\n",
        "    print(f\"\\n최적의 학습률은 {best_lr}, 최적의 에포크는 {best_epoch}입니다.\")\n",
        "    print(f\"최적의 파라미터: w = {best_w}, b = {best_b}, 최종 손실 = {best_loss:.4f}\")\n",
        "\n",
        "    return best_lr, best_epoch, best_w, best_b, best_loss\n",
        "\n",
        "# 데이터 생성용 실제 파라미터\n",
        "true_w = 2.0\n",
        "true_b = 1.0\n",
        "\n",
        "# 가짜 데이터 생성\n",
        "X = torch.randn(100, 1) * 10  # 100개의 샘플, 1개의 특징\n",
        "y = true_w * X + true_b + torch.randn(100, 1) * 2  # 노이즈 추가\n",
        "\n",
        "# 테스트할 학습률 목록\n",
        "learning_rates = [0.001, 0.005, 0.01]\n",
        "\n",
        "# 최적의 학습률과 에포크 찾기\n",
        "best_lr, best_epoch, best_w, best_b, best_loss = find_best_learning_rate_and_epoch(X, y, learning_rates, epoch_range=1000)\n",
        "\n",
        "# 최적의 학습률과 에포크로 모델 재학습\n",
        "print(f\"\\n최적의 학습률: {best_lr}, 최적의 에포크: {best_epoch}로 모델 재학습 중...\")\n",
        "final_w, final_b, final_loss, _ = train_linear_regression(X, y, learning_rate=best_lr, epochs=best_epoch)\n",
        "print(f\"\\n최종 모델: w = {final_w}, b = {final_b}, 최종 손실 = {final_loss:.4f}\")"
      ],
      "metadata": {
        "id": "dDFD6NNUuW5S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ec149ce-fc91-4824-c8d5-974da3cac811"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "학습률: 0.001로 학습 중\n",
            "최종 파라미터: w = 1.9917962551116943, b = 1.05576491355896, 최종 손실 = 3.8711, 최적 에포크 = 999\n",
            "\n",
            "학습률: 0.005로 학습 중\n",
            "최종 파라미터: w = 1.9906895160675049, b = 0.9460059404373169, 최종 손실 = 3.8591, 최적 에포크 = 787\n",
            "\n",
            "학습률: 0.01로 학습 중\n",
            "최종 파라미터: w = 1.9906901121139526, b = 0.9460626244544983, 최종 손실 = 3.8591, 최적 에포크 = 565\n",
            "\n",
            "최적의 학습률은 0.005, 최적의 에포크는 787입니다.\n",
            "최적의 파라미터: w = 1.9906895160675049, b = 0.9460059404373169, 최종 손실 = 3.8591\n",
            "\n",
            "최적의 학습률: 0.005, 최적의 에포크: 787로 모델 재학습 중...\n",
            "\n",
            "최종 모델: w = 1.9906829595565796, b = 0.9453567266464233, 최종 손실 = 3.8591\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Teacher's Code\n",
        "import random\n",
        "\n",
        "def generate_artificial_data(true_w, true_b, n_data,\n",
        "                                x_amplitude = 10,\n",
        "                                noise_amplitude = 0.2):\n",
        "    \"\"\"Generate data from y = true_w * x + true_b\n",
        "    \"\"\"\n",
        "    X = torch.randn(n_data, 1) * x_amplitude\n",
        "    y = true_w * X + true_b + torch.randn(n_data, 1) * noise_amplitude\n",
        "    # 노이즈를 섞어줌\n",
        "\n",
        "    return X, y\n",
        "\n",
        "def linear_regression(data, learning_rate, epochs, queit = False):\n",
        "    X, y = data\n",
        "    w = torch.randn(1, 1, requires_grad=True)\n",
        "    b = torch.randn(1, requires_grad=True)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        # Forward pass: compute predicted y\n",
        "        # 100, 1 / 1 -> 100, 1 / 1, 1 -> 100, 1 / 100, 1\n",
        "        y_pred = X * w + b # y_pred: 100, 1\n",
        "\n",
        "        # Compute and print loss\n",
        "        loss = torch.mean((y_pred - y) ** 2) # 100, 1\n",
        "\n",
        "        if epoch % 100 == 0 and not queit:\n",
        "            print(f'Epoch {epoch}: w = {w.item():.4f}, b = {b.item():.4f}, loss = {loss.item():.4f}')\n",
        "        # Backward pass: compute gradients\n",
        "        loss.backward()\n",
        "        # 미분하는 부분\n",
        "\n",
        "        # Update parameters using gradient descent\n",
        "        with torch.no_grad():\n",
        "            w -= learning_rate * w.grad\n",
        "            b -= learning_rate * b.grad\n",
        "\n",
        "        # Zero gradients after updating\n",
        "        w.grad.zero_()\n",
        "        b.grad.zero_()\n",
        "        # zero를 안하면 ?\n",
        "\n",
        "    return w, b\n",
        "\n",
        "data = generate_artificial_data(2.0, 1.0, 1000)\n",
        "linear_regression(data, 0.001, 1000)\n",
        "\n",
        "def evaluate_model(true_w, true_b, w, b):\n",
        "    return (true_w - w) ** 2 / true_w ** 2 + (true_b - b) ** 2 / true_b ** 2\n",
        "\n",
        "def find_learning_rate():\n",
        "    true_w = random.gauss(0, 10)\n",
        "    true_b = random.gauss(0, 10)\n",
        "\n",
        "    data = generate_artificial_data(true_w, true_b, 1000)\n",
        "\n",
        "    for learning_rate in [0.0001 * (i+1) for i in range(10)]:\n",
        "        w, b = linear_regression(data, learning_rate, 1000, queit = True)\n",
        "        score = evaluate_model(true_w, true_b, w, b)\n",
        "        print(score, learning_rate)\n",
        "\n",
        "find_learning_rate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3lU39SNMPlw",
        "outputId": "3283a773-71bf-4848-d460-8bba73e14e0f"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: w = -0.1624, b = -0.2387, loss = 437.1707\n",
            "Epoch 100: w = 1.9997, b = -0.0151, loss = 1.0449\n",
            "Epoch 200: w = 1.9996, b = 0.1670, loss = 0.7125\n",
            "Epoch 300: w = 1.9995, b = 0.3160, loss = 0.4898\n",
            "Epoch 400: w = 1.9995, b = 0.4381, loss = 0.3406\n",
            "Epoch 500: w = 1.9994, b = 0.5379, loss = 0.2406\n",
            "Epoch 600: w = 1.9994, b = 0.6197, loss = 0.1736\n",
            "Epoch 700: w = 1.9993, b = 0.6866, loss = 0.1287\n",
            "Epoch 800: w = 1.9993, b = 0.7414, loss = 0.0986\n",
            "Epoch 900: w = 1.9993, b = 0.7862, loss = 0.0785\n",
            "tensor([[0.9035]], grad_fn=<AddBackward0>) 0.0001\n",
            "tensor([[0.4352]], grad_fn=<AddBackward0>) 0.0002\n",
            "tensor([[0.2619]], grad_fn=<AddBackward0>) 0.00030000000000000003\n",
            "tensor([[0.2246]], grad_fn=<AddBackward0>) 0.0004\n",
            "tensor([[0.1312]], grad_fn=<AddBackward0>) 0.0005\n",
            "tensor([[0.1035]], grad_fn=<AddBackward0>) 0.0006000000000000001\n",
            "tensor([[0.0532]], grad_fn=<AddBackward0>) 0.0007\n",
            "tensor([[0.0458]], grad_fn=<AddBackward0>) 0.0008\n",
            "tensor([[0.0238]], grad_fn=<AddBackward0>) 0.0009000000000000001\n",
            "tensor([[0.0119]], grad_fn=<AddBackward0>) 0.001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 선형회귀 조금 더 해보기\n"
      ],
      "metadata": {
        "id": "XlzOMd44rinI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO: 이번에는 비슷하게, 입력이 3개이고 출력이 1개인 선형회귀를 해 보자.\n",
        "\n",
        "$y = w_1 x_1 + w_2 x_2 + w_3 x_3 + b$"
      ],
      "metadata": {
        "id": "pgue321swjEE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# y = w1*x1 + w2 * x2 + w3 * x3 + b\n",
        "import random\n",
        "\n",
        "def generate_artificial_data(true_w, true_b, n_data,\n",
        "                                x_amplitude = 10,\n",
        "                                noise_amplitude = 0.2):\n",
        "    \"\"\"Generate data from y = true_w1 * x1 + true_w2 * x2 + true_w3 * x3 +true_b\n",
        "    \"\"\"\n",
        "    # Generated x1, x2, x3\n",
        "    X = torch.randn(n_data, 3) * x_amplitude\n",
        "    y = true_w[0] * X[:, 0:1] + true_w[1] * X[:, 1:2] + true_w[2] * X[:, 2:3] + true_b\n",
        "\n",
        "    # Noise\n",
        "    y += torch.randn(n_data, 1) * noise_amplitude\n",
        "\n",
        "    return X, y\n",
        "\n",
        "def linear_regression(data, learning_rate, epochs, quiet = False):\n",
        "    X, y = data\n",
        "    w = torch.randn(3, 1, requires_grad=True)\n",
        "    b = torch.randn(1, requires_grad=True)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        # Forward pass: compute predicted y\n",
        "        # 100, 1 / 1 -> 100, 1 / 1, 1 -> 100, 1 / 100, 1\n",
        "        y_pred = X @ w + b # y_pred: 100, 1\n",
        "\n",
        "        # Compute and print loss\n",
        "        loss = torch.mean((y_pred - y) ** 2) # 100, 1\n",
        "\n",
        "        if epoch % 100 == 0 and not quiet:\n",
        "            print(f'Epoch {epoch}: loss = {loss.item():.4f}')\n",
        "        # Backward pass: compute gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # Update parameters using gradient descent\n",
        "        with torch.no_grad():\n",
        "            w -= learning_rate * w.grad\n",
        "            b -= learning_rate * b.grad\n",
        "\n",
        "        # Zero gradients after updating\n",
        "        w.grad.zero_()\n",
        "        b.grad.zero_()\n",
        "\n",
        "    return w, b\n",
        "\n",
        "data = generate_artificial_data([2.0, 2.0, 2.0], 1.0, 1000)\n",
        "linear_regression(data, 0.001, 1000)\n",
        "\n",
        "def evaluate_model(true_w, true_b, w, b):\n",
        "    return torch.mean((true_w - w)**2 / true_w**2) + (true_b - b)**2 / true_b**2\n",
        "\n",
        "def find_learning_rate():\n",
        "    true_w =  [random.gauss(0, 10) for _ in range(3)]\n",
        "    true_b = random.gauss(0, 10)\n",
        "\n",
        "    data = generate_artificial_data(true_w, true_b, 1000)\n",
        "\n",
        "    for learning_rate in [0.0001 * (i+1) for i in range(10)]:\n",
        "        w, b = linear_regression(data, learning_rate, 1000, quiet = True)\n",
        "        score = evaluate_model(torch.tensor(true_w).reshape(-1, 1), true_b, w, b)\n",
        "        print(f\"Model evaluation score: {score.item():.6f} | Learning rate: {learning_rate:.6f}\")\n",
        "\n",
        "find_learning_rate()"
      ],
      "metadata": {
        "id": "Pe5Uqs91xjtC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5ce2cf4-1a84-48c4-d770-9ccfdd9bdf63"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: loss = 229.2000\n",
            "Epoch 100: loss = 0.0568\n",
            "Epoch 200: loss = 0.0518\n",
            "Epoch 300: loss = 0.0484\n",
            "Epoch 400: loss = 0.0462\n",
            "Epoch 500: loss = 0.0446\n",
            "Epoch 600: loss = 0.0436\n",
            "Epoch 700: loss = 0.0430\n",
            "Epoch 800: loss = 0.0425\n",
            "Epoch 900: loss = 0.0422\n",
            "Model evaluation score: 0.546923 | Learning rate: 0.000100\n",
            "Model evaluation score: 0.426096 | Learning rate: 0.000200\n",
            "Model evaluation score: 0.306964 | Learning rate: 0.000300\n",
            "Model evaluation score: 0.204569 | Learning rate: 0.000400\n",
            "Model evaluation score: 0.180114 | Learning rate: 0.000500\n",
            "Model evaluation score: 0.090582 | Learning rate: 0.000600\n",
            "Model evaluation score: 0.069178 | Learning rate: 0.000700\n",
            "Model evaluation score: 0.054504 | Learning rate: 0.000800\n",
            "Model evaluation score: 0.037414 | Learning rate: 0.000900\n",
            "Model evaluation score: 0.024988 | Learning rate: 0.001000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 뒤에서 할 내용 미리 살짝 엿보기 - Optimizer\n"
      ],
      "metadata": {
        "id": "ROZ96vR1wI5h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 이번에는 adam optimizer를 한번 사용해보자.\n",
        "\n",
        "import torch\n",
        "import math\n",
        "\n",
        "# 임의로 데이터를 한번 만들어 보자.\n",
        "# True parameters\n",
        "true_w = 2.0\n",
        "true_b = 1.0\n",
        "\n",
        "# Generate data\n",
        "X = torch.randn(100, 1) * 10  # 100 samples, single feature\n",
        "y = true_w * X + true_b + torch.randn(100, 1) * 2  # Add noise\n",
        "\n",
        "# requires_grad = True로 해야 학습이 가능\n",
        "w = torch.randn(1, 1, requires_grad=True)\n",
        "b = torch.randn(1, requires_grad=True)\n",
        "\n",
        "# 아담 옵티마이저 하이퍼파라미터 설정\n",
        "learning_rate = 0.005\n",
        "epochs = 10000\n",
        "beta1 = 0.9\n",
        "beta2 = 0.999\n",
        "epsilon = 1e-8\n",
        "\n",
        "# 아담 옵티마이저를 위한 모멘트 변수 초기화\n",
        "m_w = torch.zeros_like(w)\n",
        "v_w = torch.zeros_like(w)\n",
        "m_b = torch.zeros_like(b)\n",
        "v_b = torch.zeros_like(b)\n",
        "\n",
        "# 아담 옵티마이저를 위한 시간 스텝 변수 초기화\n",
        "t = 0\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    # Forward pass: compute predicted y\n",
        "    y_pred = X * w + b\n",
        "\n",
        "    # Compute and print loss\n",
        "    loss = torch.mean((y_pred - y) ** 2)\n",
        "\n",
        "    # Backward pass: compute gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # 아담 옵티마이저 업데이트\n",
        "    with torch.no_grad():\n",
        "        t += 1  # 시간 스텝 증가\n",
        "\n",
        "        # w 파라미터 업데이트\n",
        "        m_w = beta1 * m_w + (1 - beta1) * w.grad\n",
        "        v_w = beta2 * v_w + (1 - beta2) * (w.grad ** 2)\n",
        "        # 편향 보정\n",
        "        m_w_hat = m_w / (1 - beta1 ** t)\n",
        "        v_w_hat = v_w / (1 - beta2 ** t)\n",
        "        # 파라미터 업데이트\n",
        "        w -= learning_rate * m_w_hat / (torch.sqrt(v_w_hat) + epsilon)\n",
        "\n",
        "        # b 파라미터 업데이트\n",
        "        m_b = beta1 * m_b + (1 - beta1) * b.grad\n",
        "        v_b = beta2 * v_b + (1 - beta2) * (b.grad ** 2)\n",
        "        # 편향 보정\n",
        "        m_b_hat = m_b / (1 - beta1 ** t)\n",
        "        v_b_hat = v_b / (1 - beta2 ** t)\n",
        "        # 파라미터 업데이트\n",
        "        b -= learning_rate * m_b_hat / (torch.sqrt(v_b_hat) + epsilon)\n",
        "\n",
        "    # Gradients 초기화\n",
        "    w.grad.zero_()\n",
        "    b.grad.zero_()\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f'Epoch {epoch}: w = {w.item():.4f}, b = {b.item():.4f}, loss = {loss.item():.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bR7URmNB0e7n",
        "outputId": "0b07f13c-abc8-426c-bdfa-97124d5bf7a5"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 100: w = -0.1918, b = -0.9169, loss = 397.5360\n",
            "Epoch 200: w = 0.2413, b = -1.0164, loss = 257.1836\n",
            "Epoch 300: w = 0.6212, b = -0.8271, loss = 159.6189\n",
            "Epoch 400: w = 0.9467, b = -0.5291, loss = 94.8223\n",
            "Epoch 500: w = 1.2180, b = -0.2370, loss = 54.1046\n",
            "Epoch 600: w = 1.4370, b = 0.0116, loss = 30.0688\n",
            "Epoch 700: w = 1.6076, b = 0.2091, loss = 16.8211\n",
            "Epoch 800: w = 1.7353, b = 0.3582, loss = 10.0388\n",
            "Epoch 900: w = 1.8271, b = 0.4658, loss = 6.8271\n",
            "Epoch 1000: w = 1.8904, b = 0.5400, loss = 5.4252\n",
            "Epoch 1100: w = 1.9320, b = 0.5888, loss = 4.8627\n",
            "Epoch 1200: w = 1.9582, b = 0.6196, loss = 4.6558\n",
            "Epoch 1300: w = 1.9740, b = 0.6381, loss = 4.5862\n",
            "Epoch 1400: w = 1.9830, b = 0.6487, loss = 4.5649\n",
            "Epoch 1500: w = 1.9879, b = 0.6545, loss = 4.5589\n",
            "Epoch 1600: w = 1.9904, b = 0.6575, loss = 4.5574\n",
            "Epoch 1700: w = 1.9917, b = 0.6590, loss = 4.5571\n",
            "Epoch 1800: w = 1.9923, b = 0.6596, loss = 4.5570\n",
            "Epoch 1900: w = 1.9925, b = 0.6599, loss = 4.5570\n",
            "Epoch 2000: w = 1.9926, b = 0.6601, loss = 4.5570\n",
            "Epoch 2100: w = 1.9927, b = 0.6601, loss = 4.5570\n",
            "Epoch 2200: w = 1.9927, b = 0.6601, loss = 4.5570\n",
            "Epoch 2300: w = 1.9927, b = 0.6601, loss = 4.5570\n",
            "Epoch 2400: w = 1.9927, b = 0.6601, loss = 4.5570\n",
            "Epoch 2500: w = 1.9927, b = 0.6601, loss = 4.5570\n",
            "Epoch 2600: w = 1.9927, b = 0.6601, loss = 4.5570\n",
            "Epoch 2700: w = 1.9927, b = 0.6601, loss = 4.5570\n",
            "Epoch 2800: w = 1.9927, b = 0.6601, loss = 4.5570\n",
            "Epoch 2900: w = 1.9927, b = 0.6601, loss = 4.5570\n",
            "Epoch 3000: w = 1.9927, b = 0.6601, loss = 4.5570\n",
            "Epoch 3100: w = 1.9927, b = 0.6601, loss = 4.5570\n",
            "Epoch 3200: w = 1.9927, b = 0.6601, loss = 4.5570\n",
            "Epoch 3300: w = 1.9927, b = 0.6601, loss = 4.5570\n",
            "Epoch 3400: w = 1.9927, b = 0.6601, loss = 4.5570\n",
            "Epoch 3500: w = 1.9927, b = 0.6601, loss = 4.5570\n",
            "Epoch 3600: w = 1.9927, b = 0.6601, loss = 4.5570\n",
            "Epoch 3700: w = 1.9927, b = 0.6601, loss = 4.5570\n",
            "Epoch 3800: w = 1.9927, b = 0.6601, loss = 4.5570\n",
            "Epoch 3900: w = 1.9927, b = 0.6601, loss = 4.5570\n",
            "Epoch 4000: w = 1.9927, b = 0.6601, loss = 4.5570\n",
            "Epoch 4100: w = 1.9927, b = 0.6601, loss = 4.5570\n",
            "Epoch 4200: w = 1.9927, b = 0.6601, loss = 4.5570\n",
            "Epoch 4300: w = 1.9927, b = 0.6601, loss = 4.5570\n",
            "Epoch 4400: w = 1.9927, b = 0.6601, loss = 4.5570\n",
            "Epoch 4500: w = 1.9927, b = 0.6601, loss = 4.5570\n",
            "Epoch 4600: w = 1.9927, b = 0.6601, loss = 4.5570\n",
            "Epoch 4700: w = 1.9927, b = 0.6601, loss = 4.5570\n",
            "Epoch 4800: w = 1.9927, b = 0.6601, loss = 4.5570\n",
            "Epoch 4900: w = 1.9927, b = 0.6601, loss = 4.5570\n",
            "Epoch 5000: w = 1.9927, b = 0.6601, loss = 4.5570\n",
            "Epoch 5100: w = 1.9927, b = 0.6601, loss = 4.5570\n",
            "Epoch 5200: w = 1.9927, b = 0.6601, loss = 4.5570\n",
            "Epoch 5300: w = 1.9927, b = 0.6601, loss = 4.5570\n",
            "Epoch 5400: w = 1.9927, b = 0.6601, loss = 4.5570\n",
            "Epoch 5500: w = 1.9927, b = 0.6601, loss = 4.5570\n",
            "Epoch 5600: w = 1.9927, b = 0.6601, loss = 4.5570\n",
            "Epoch 5700: w = 1.9927, b = 0.6601, loss = 4.5570\n",
            "Epoch 5800: w = 1.9927, b = 0.6601, loss = 4.5570\n",
            "Epoch 5900: w = 1.9927, b = 0.6601, loss = 4.5570\n",
            "Epoch 6000: w = 1.9927, b = 0.6601, loss = 4.5570\n",
            "Epoch 6100: w = 1.9927, b = 0.6601, loss = 4.5570\n",
            "Epoch 6200: w = 1.9927, b = 0.6601, loss = 4.5570\n",
            "Epoch 6300: w = 1.9927, b = 0.6601, loss = 4.5570\n",
            "Epoch 6400: w = 1.9927, b = 0.6601, loss = 4.5570\n",
            "Epoch 6500: w = 1.9927, b = 0.6601, loss = 4.5570\n",
            "Epoch 6600: w = 1.9927, b = 0.6601, loss = 4.5570\n",
            "Epoch 6700: w = 1.9927, b = 0.6601, loss = 4.5570\n",
            "Epoch 6800: w = 1.9927, b = 0.6601, loss = 4.5570\n",
            "Epoch 6900: w = 1.9927, b = 0.6601, loss = 4.5570\n",
            "Epoch 7000: w = 1.9927, b = 0.6601, loss = 4.5570\n",
            "Epoch 7100: w = 1.9927, b = 0.6601, loss = 4.5570\n",
            "Epoch 7200: w = 1.9927, b = 0.6601, loss = 4.5570\n",
            "Epoch 7300: w = 1.9927, b = 0.6601, loss = 4.5570\n",
            "Epoch 7400: w = 1.9927, b = 0.6601, loss = 4.5570\n",
            "Epoch 7500: w = 1.9927, b = 0.6601, loss = 4.5570\n",
            "Epoch 7600: w = 1.9927, b = 0.6601, loss = 4.5570\n",
            "Epoch 7700: w = 1.9927, b = 0.6601, loss = 4.5570\n",
            "Epoch 7800: w = 1.9927, b = 0.6601, loss = 4.5570\n",
            "Epoch 7900: w = 1.9927, b = 0.6601, loss = 4.5570\n",
            "Epoch 8000: w = 1.9927, b = 0.6601, loss = 4.5570\n",
            "Epoch 8100: w = 1.9927, b = 0.6601, loss = 4.5570\n",
            "Epoch 8200: w = 1.9927, b = 0.6601, loss = 4.5570\n",
            "Epoch 8300: w = 1.9927, b = 0.6601, loss = 4.5570\n",
            "Epoch 8400: w = 1.9927, b = 0.6601, loss = 4.5570\n",
            "Epoch 8500: w = 1.9927, b = 0.6601, loss = 4.5570\n",
            "Epoch 8600: w = 1.9927, b = 0.6601, loss = 4.5570\n",
            "Epoch 8700: w = 1.9927, b = 0.6601, loss = 4.5570\n",
            "Epoch 8800: w = 1.9927, b = 0.6601, loss = 4.5570\n",
            "Epoch 8900: w = 1.9927, b = 0.6601, loss = 4.5570\n",
            "Epoch 9000: w = 1.9927, b = 0.6601, loss = 4.5570\n",
            "Epoch 9100: w = 1.9927, b = 0.6601, loss = 4.5570\n",
            "Epoch 9200: w = 1.9927, b = 0.6601, loss = 4.5570\n",
            "Epoch 9300: w = 1.9927, b = 0.6601, loss = 4.5570\n",
            "Epoch 9400: w = 1.9927, b = 0.6601, loss = 4.5570\n",
            "Epoch 9500: w = 1.9927, b = 0.6601, loss = 4.5570\n",
            "Epoch 9600: w = 1.9927, b = 0.6601, loss = 4.5570\n",
            "Epoch 9700: w = 1.9927, b = 0.6601, loss = 4.5570\n",
            "Epoch 9800: w = 1.9927, b = 0.6601, loss = 4.5570\n",
            "Epoch 9900: w = 1.9927, b = 0.6601, loss = 4.5570\n",
            "Epoch 10000: w = 1.9927, b = 0.6601, loss = 4.5570\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "자꾸 local minima 어딘가에 빠지는 것 같다. 이걸 수정하기 위해서, 일정 횟수 이상 바뀌지 않으면 noise를 주는 방식을 생각해보자.\n"
      ],
      "metadata": {
        "id": "BhIwbEiQ6meH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import math\n",
        "\n",
        "# 임의로 데이터를 한번 만들어 보자.\n",
        "# True parameters\n",
        "true_w = 2.0\n",
        "true_b = 1.0\n",
        "\n",
        "# Generate data\n",
        "torch.manual_seed(42)  # 재현성을 위해 시드 설정\n",
        "X = torch.randn(100, 1) * 10  # 100 samples, single feature\n",
        "y = true_w * X + true_b + torch.randn(100, 1) * 2  # Add noise\n",
        "\n",
        "# requires_grad = True로 해야 학습이 가능\n",
        "w = torch.randn(1, 1, requires_grad=True)\n",
        "b = torch.randn(1, requires_grad=True)\n",
        "\n",
        "# 아담 옵티마이저 하이퍼파라미터 설정\n",
        "learning_rate = 0.005\n",
        "epochs = 10000\n",
        "beta1 = 0.9\n",
        "beta2 = 0.999\n",
        "epsilon = 1e-8\n",
        "\n",
        "# 아담 옵티마이저를 위한 모멘트 변수 초기화\n",
        "m_w = torch.zeros_like(w)\n",
        "v_w = torch.zeros_like(w)\n",
        "m_b = torch.zeros_like(b)\n",
        "v_b = torch.zeros_like(b)\n",
        "\n",
        "# 아담 옵티마이저를 위한 시간 스텝 변수 초기화\n",
        "t = 0\n",
        "\n",
        "\n",
        "patience = 300  # 손실과 파라미터 변화가 임계값 이하로 유지되는 에포크 수\n",
        "threshold_loss = 1e-4  # 손실 변화 임계값\n",
        "threshold_w = 1e-4     # w 변화 임계값\n",
        "threshold_b = 1e-4     # b 변화 임계값\n",
        "\n",
        "\n",
        "loss_history = []\n",
        "w_history = []\n",
        "b_history = []\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    # Forward pass: compute predicted y\n",
        "    y_pred = X * w + b\n",
        "\n",
        "    # Compute and print loss\n",
        "    loss = torch.mean((y_pred - y) ** 2)\n",
        "\n",
        "    # Backward pass: compute gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # 아담 옵티마이저 업데이트\n",
        "    with torch.no_grad():\n",
        "        t += 1  # 시간 스텝 증가\n",
        "\n",
        "        # w 파라미터 업데이트\n",
        "        m_w = beta1 * m_w + (1 - beta1) * w.grad\n",
        "        v_w = beta2 * v_w + (1 - beta2) * (w.grad ** 2)\n",
        "        # 편향 보정\n",
        "        m_w_hat = m_w / (1 - beta1 ** t)\n",
        "        v_w_hat = v_w / (1 - beta2 ** t)\n",
        "        # 파라미터 업데이트\n",
        "        w -= learning_rate * m_w_hat / (torch.sqrt(v_w_hat) + epsilon)\n",
        "\n",
        "        # b 파라미터 업데이트\n",
        "        m_b = beta1 * m_b + (1 - beta1) * b.grad\n",
        "        v_b = beta2 * v_b + (1 - beta2) * (b.grad ** 2)\n",
        "        # 편향 보정\n",
        "        m_b_hat = m_b / (1 - beta1 ** t)\n",
        "        v_b_hat = v_b / (1 - beta2 ** t)\n",
        "        # 파라미터 업데이트\n",
        "        b -= learning_rate * m_b_hat / (torch.sqrt(v_b_hat) + epsilon)\n",
        "\n",
        "    # Gradients 초기화\n",
        "    w.grad.zero_()\n",
        "    b.grad.zero_()\n",
        "\n",
        "    # 손실과 파라미터 값을 기록\n",
        "    loss_history.append(loss.item())\n",
        "    w_history.append(w.item())\n",
        "    b_history.append(b.item())\n",
        "\n",
        "    # Patience에 도달했는지 확인\n",
        "    if epoch >= patience :\n",
        "        # 최근 'patience' 에포크의 손실 변화 계산\n",
        "        recent_losses = loss_history[-patience:]\n",
        "        loss_deltas = [abs(recent_losses[i] - recent_losses[i-1]) for i in range(1, patience)]\n",
        "        max_loss_delta = max(loss_deltas)\n",
        "\n",
        "        # 최근 'patience' 에포크의 w 변화 계산\n",
        "        recent_ws = w_history[-patience:]\n",
        "        w_deltas = [abs(recent_ws[i] - recent_ws[i-1]) for i in range(1, patience)]\n",
        "        max_w_delta = max(w_deltas)\n",
        "\n",
        "        # 최근 'patience' 에포크의 b 변화 계산\n",
        "        recent_bs = b_history[-patience:]\n",
        "        b_deltas = [abs(recent_bs[i] - recent_bs[i-1]) for i in range(1, patience)]\n",
        "        max_b_delta = max(b_deltas)\n",
        "\n",
        "        # 변화가 모두 임계값 이하인 경우 노이즈 추가\n",
        "        if (max_loss_delta < threshold_loss) and (max_w_delta < threshold_w) and (max_b_delta < threshold_b):\n",
        "            print(f'\\nEpoch {epoch}: No significant updates in the last {patience} epochs. Adding noise to parameters.')\n",
        "            # 파라미터에 노이즈 추가\n",
        "            noise_w = torch.randn_like(w) * 0.1\n",
        "            noise_b = torch.randn_like(b) * 0.1\n",
        "            w.data += noise_w\n",
        "            b.data += noise_b\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f'Epoch {epoch}: w = {w.item():.4f}, b = {b.item():.4f}, loss = {loss.item():.4f}')\n",
        "\n",
        "# 최종 파라미터 출력\n",
        "print(f'\\nFinal Parameters: w = {w.item():.4f}, b = {b.item():.4f}, loss = {loss.item():.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-JPhYZV4PXg",
        "outputId": "3b7314e7-3c4a-4eee-8064-3fc01bea8a89"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 100: w = 0.8642, b = 0.2516, loss = 131.1085\n",
            "Epoch 200: w = 1.2482, b = 0.6152, loss = 59.2300\n",
            "Epoch 300: w = 1.5384, b = 0.8670, loss = 24.3059\n",
            "Epoch 400: w = 1.7397, b = 1.0158, loss = 9.8937\n",
            "Epoch 500: w = 1.8663, b = 1.0852, loss = 4.9355\n",
            "Epoch 600: w = 1.9380, b = 1.1053, loss = 3.5270\n",
            "Epoch 700: w = 1.9746, b = 1.1020, loss = 3.1972\n",
            "Epoch 800: w = 1.9914, b = 1.0920, loss = 3.1333\n",
            "Epoch 900: w = 1.9985, b = 1.0830, loss = 3.1230\n",
            "Epoch 1000: w = 2.0011, b = 1.0771, loss = 3.1216\n",
            "Epoch 1100: w = 2.0020, b = 1.0738, loss = 3.1215\n",
            "\n",
            "Epoch 1143: No significant updates in the last 300 epochs. Adding noise to parameters.\n",
            "Epoch 1200: w = 2.0690, b = 1.0295, loss = 3.5617\n",
            "Epoch 1300: w = 2.0176, b = 1.0432, loss = 3.1449\n",
            "Epoch 1400: w = 2.0055, b = 1.0597, loss = 3.1225\n",
            "Epoch 1500: w = 2.0029, b = 1.0675, loss = 3.1215\n",
            "Epoch 1600: w = 2.0025, b = 1.0703, loss = 3.1214\n",
            "Epoch 1700: w = 2.0024, b = 1.0711, loss = 3.1214\n",
            "\n",
            "Epoch 1719: No significant updates in the last 300 epochs. Adding noise to parameters.\n",
            "Epoch 1800: w = 1.9932, b = 1.0648, loss = 3.1300\n",
            "Epoch 1900: w = 2.0016, b = 1.0719, loss = 3.1215\n",
            "Epoch 2000: w = 2.0023, b = 1.0716, loss = 3.1214\n",
            "Epoch 2100: w = 2.0024, b = 1.0714, loss = 3.1214\n",
            "\n",
            "Epoch 2132: No significant updates in the last 300 epochs. Adding noise to parameters.\n",
            "Epoch 2200: w = 1.9996, b = 1.1031, loss = 3.1232\n",
            "Epoch 2300: w = 2.0022, b = 1.0747, loss = 3.1215\n",
            "Epoch 2400: w = 2.0024, b = 1.0716, loss = 3.1214\n",
            "Epoch 2500: w = 2.0024, b = 1.0714, loss = 3.1214\n",
            "\n",
            "Epoch 2589: No significant updates in the last 300 epochs. Adding noise to parameters.\n",
            "Epoch 2600: w = 2.0541, b = 0.9360, loss = 3.4015\n",
            "Epoch 2700: w = 2.0030, b = 1.0600, loss = 3.1216\n",
            "Epoch 2800: w = 2.0024, b = 1.0710, loss = 3.1214\n",
            "Epoch 2900: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "Epoch 3000: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "\n",
            "Epoch 3040: No significant updates in the last 300 epochs. Adding noise to parameters.\n",
            "Epoch 3100: w = 1.9999, b = 1.1043, loss = 3.1232\n",
            "Epoch 3200: w = 2.0024, b = 1.0714, loss = 3.1214\n",
            "Epoch 3300: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "Epoch 3400: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "\n",
            "Epoch 3459: No significant updates in the last 300 epochs. Adding noise to parameters.\n",
            "Epoch 3500: w = 2.0024, b = 1.0742, loss = 3.1215\n",
            "Epoch 3600: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "Epoch 3700: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "Epoch 3800: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "\n",
            "Epoch 3811: No significant updates in the last 300 epochs. Adding noise to parameters.\n",
            "Epoch 3900: w = 2.0015, b = 1.0751, loss = 3.1216\n",
            "Epoch 4000: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "Epoch 4100: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "Epoch 4200: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "\n",
            "Epoch 4227: No significant updates in the last 300 epochs. Adding noise to parameters.\n",
            "Epoch 4300: w = 2.0040, b = 1.0679, loss = 3.1218\n",
            "Epoch 4400: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "Epoch 4500: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "Epoch 4600: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "\n",
            "Epoch 4635: No significant updates in the last 300 epochs. Adding noise to parameters.\n",
            "Epoch 4700: w = 1.9985, b = 1.0766, loss = 3.1234\n",
            "Epoch 4800: w = 2.0024, b = 1.0715, loss = 3.1214\n",
            "Epoch 4900: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "Epoch 5000: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "\n",
            "Epoch 5044: No significant updates in the last 300 epochs. Adding noise to parameters.\n",
            "Epoch 5100: w = 2.0087, b = 1.0583, loss = 3.1268\n",
            "Epoch 5200: w = 2.0023, b = 1.0712, loss = 3.1214\n",
            "Epoch 5300: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "Epoch 5400: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "\n",
            "Epoch 5467: No significant updates in the last 300 epochs. Adding noise to parameters.\n",
            "Epoch 5500: w = 2.0052, b = 1.0815, loss = 3.1223\n",
            "Epoch 5600: w = 2.0023, b = 1.0713, loss = 3.1214\n",
            "Epoch 5700: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "Epoch 5800: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "\n",
            "Epoch 5830: No significant updates in the last 300 epochs. Adding noise to parameters.\n",
            "Epoch 5900: w = 2.0051, b = 1.0725, loss = 3.1224\n",
            "Epoch 6000: w = 2.0024, b = 1.0714, loss = 3.1214\n",
            "Epoch 6100: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "Epoch 6200: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "\n",
            "Epoch 6236: No significant updates in the last 300 epochs. Adding noise to parameters.\n",
            "Epoch 6300: w = 2.0013, b = 1.0746, loss = 3.1216\n",
            "Epoch 6400: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "Epoch 6500: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "Epoch 6600: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "\n",
            "Epoch 6627: No significant updates in the last 300 epochs. Adding noise to parameters.\n",
            "Epoch 6700: w = 2.0038, b = 1.0747, loss = 3.1217\n",
            "Epoch 6800: w = 2.0024, b = 1.0714, loss = 3.1214\n",
            "Epoch 6900: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "Epoch 7000: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "\n",
            "Epoch 7034: No significant updates in the last 300 epochs. Adding noise to parameters.\n",
            "Epoch 7100: w = 2.0048, b = 1.0767, loss = 3.1220\n",
            "Epoch 7200: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "Epoch 7300: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "Epoch 7400: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "\n",
            "Epoch 7453: No significant updates in the last 300 epochs. Adding noise to parameters.\n",
            "Epoch 7500: w = 1.9919, b = 1.0830, loss = 3.1343\n",
            "Epoch 7600: w = 2.0024, b = 1.0716, loss = 3.1214\n",
            "Epoch 7700: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "Epoch 7800: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "\n",
            "Epoch 7885: No significant updates in the last 300 epochs. Adding noise to parameters.\n",
            "Epoch 7900: w = 1.9786, b = 1.0225, loss = 3.1576\n",
            "Epoch 8000: w = 2.0025, b = 1.0722, loss = 3.1214\n",
            "Epoch 8100: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "Epoch 8200: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "\n",
            "Epoch 8295: No significant updates in the last 300 epochs. Adding noise to parameters.\n",
            "Epoch 8300: w = 2.0005, b = 1.0024, loss = 3.1283\n",
            "Epoch 8400: w = 2.0023, b = 1.0717, loss = 3.1214\n",
            "Epoch 8500: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "Epoch 8600: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "\n",
            "Epoch 8687: No significant updates in the last 300 epochs. Adding noise to parameters.\n",
            "Epoch 8700: w = 2.0347, b = 1.1205, loss = 3.1790\n",
            "Epoch 8800: w = 2.0022, b = 1.0708, loss = 3.1214\n",
            "Epoch 8900: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "Epoch 9000: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "Epoch 9100: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "\n",
            "Epoch 9103: No significant updates in the last 300 epochs. Adding noise to parameters.\n",
            "Epoch 9200: w = 2.0033, b = 1.0751, loss = 3.1215\n",
            "Epoch 9300: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "Epoch 9400: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "Epoch 9500: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "\n",
            "Epoch 9530: No significant updates in the last 300 epochs. Adding noise to parameters.\n",
            "Epoch 9600: w = 2.0013, b = 1.0695, loss = 3.1215\n",
            "Epoch 9700: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "Epoch 9800: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "Epoch 9900: w = 2.0024, b = 1.0713, loss = 3.1214\n",
            "\n",
            "Epoch 9914: No significant updates in the last 300 epochs. Adding noise to parameters.\n",
            "Epoch 10000: w = 2.0019, b = 1.0707, loss = 3.1215\n",
            "\n",
            "Final Parameters: w = 2.0019, b = 1.0707, loss = 3.1215\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 딥러닝 들어가기\n",
        "\n",
        "\n",
        "아래 코드는 pytorch 에서 딥러닝 모델을 짤 때, 가장 일반적인 형식이라고 할 수 있다. 각 부분에서 쓰일 함수들은 문제에 따라서 다르지만, 대개의 경우 위 내용이 크게 바뀌지 않을 것임."
      ],
      "metadata": {
        "id": "ddcrBxNp7LCB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "# Seed for reproducibility\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)"
      ],
      "metadata": {
        "id": "uUTgBrCI7k8X"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate synthetic data\n",
        "num_samples = 1000\n",
        "input_features = 20\n",
        "\n",
        "# Features: random numbers\n",
        "X = np.random.randn(num_samples, input_features).astype(np.float32)\n",
        "\n",
        "# Labels: sum of features > 0 => class 1, else class 0\n",
        "y = (X.sum(axis=1) > 0).astype(np.float32)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_tensor = torch.from_numpy(X)\n",
        "y_tensor = torch.from_numpy(y).unsqueeze(1)  # Add dimension for compatibility\n",
        "\n",
        "# Create a dataset and data loader\n",
        "dataset = TensorDataset(X_tensor, y_tensor)\n",
        "batch_size = 32\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "AuEAYWDf7h6d"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleMLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SimpleMLP, self).__init__()\n",
        "        # Define layers\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)  # First fully connected layer\n",
        "        self.relu = nn.ReLU()                         # ReLU activation\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size) # Second fully connected layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)      # Input to first layer\n",
        "        out = self.relu(out)   # Apply ReLU\n",
        "        out = self.fc2(out)    # Output layer\n",
        "        return out\n",
        "\n",
        "# Hyperparameters\n",
        "input_size = input_features\n",
        "hidden_size = 64\n",
        "output_size = 1  # Binary classification\n",
        "\n",
        "# Instantiate the model\n",
        "model = SimpleMLP(input_size, hidden_size, output_size)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.BCEWithLogitsLoss()  # Combines a sigmoid layer and the BCELoss\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training parameters\n",
        "num_epochs = 20\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_X, batch_y in dataloader:\n",
        "        # Forward pass\n",
        "        outputs = model(batch_X)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()  # Clear gradients\n",
        "        loss.backward()        # Compute gradients\n",
        "        optimizer.step()       # Update weights\n",
        "\n",
        "    # Print loss for every epoch\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_tensor)\n",
        "    predictions = torch.sigmoid(outputs)  # Apply sigmoid to get probabilities\n",
        "    predicted_classes = (predictions >= 0.5).float()\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = (predicted_classes == y_tensor).float().mean()\n",
        "    print(f'Accuracy: {accuracy * 100:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXe436j_7Uda",
        "outputId": "de298b54-17be-4e31-a011-9e5eaa7877ad"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Loss: 0.6249\n",
            "Epoch [2/20], Loss: 0.5482\n",
            "Epoch [3/20], Loss: 0.4171\n",
            "Epoch [4/20], Loss: 0.4820\n",
            "Epoch [5/20], Loss: 0.4052\n",
            "Epoch [6/20], Loss: 0.2625\n",
            "Epoch [7/20], Loss: 0.3147\n",
            "Epoch [8/20], Loss: 0.1852\n",
            "Epoch [9/20], Loss: 0.1286\n",
            "Epoch [10/20], Loss: 0.1301\n",
            "Epoch [11/20], Loss: 0.0808\n",
            "Epoch [12/20], Loss: 0.1353\n",
            "Epoch [13/20], Loss: 0.1846\n",
            "Epoch [14/20], Loss: 0.1222\n",
            "Epoch [15/20], Loss: 0.0643\n",
            "Epoch [16/20], Loss: 0.0972\n",
            "Epoch [17/20], Loss: 0.0886\n",
            "Epoch [18/20], Loss: 0.0872\n",
            "Epoch [19/20], Loss: 0.1103\n",
            "Epoch [20/20], Loss: 0.1142\n",
            "Accuracy: 99.90%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO: input_size, hidden_size, learning_rate 등의 하이퍼파라미터를 바꿔 가며 최적의 모델을 찾아보세요."
      ],
      "metadata": {
        "id": "Vq5n0jb4vxWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write your code here"
      ],
      "metadata": {
        "id": "6YxkejqAw8--"
      },
      "execution_count": 76,
      "outputs": []
    }
  ]
}